import os
import wandb
from argparse import ArgumentParser
import numpy as np
import data_utils
from train_utils import Trainer, Evaluator # å‡è®¾ Trainer, Evaluator, get_optims åœ¨ train_utils ä¸­
import utils
from easydict import EasyDict
import yaml
import torch
from torch import nn
from arch_model_loader import load_arch_model # å¯¼å…¥æˆ‘ä»¬åˆšåˆ›å»ºçš„æ¨¡å‹åŠ è½½å™¨
import time
from tqdm import tqdm 

# -----------------------------------------------------------------------
# è¾…åŠ©å‡½æ•°ï¼šä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨ (ä»æ‚¨åŸ main.py å¤åˆ¶è¿‡æ¥)
# -----------------------------------------------------------------------
def get_optims(net, cfg, phase_idx):
    if cfg.optimizer[phase_idx] == 'adam':
        optimizer = torch.optim.Adam(net.parameters(), lr=float(cfg.lr[phase_idx]), weight_decay=float(cfg.wd[phase_idx]))
    elif cfg.optimizer[phase_idx] == 'sgd':
        optimizer = torch.optim.SGD(net.parameters(), float(cfg.lr[phase_idx]), momentum=0.9, weight_decay=float(cfg.wd[phase_idx]))

    scheduler = None
    if cfg.scheduler[phase_idx] == 'step':
        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=cfg.scheduler_step[phase_idx], gamma=cfg.scheduler_gamma[phase_idx])
    elif cfg.scheduler[phase_idx] == 'multistep':
        scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=cfg.scheduler_step[phase_idx], gamma=cfg.scheduler_gamma[phase_idx])
    elif cfg.scheduler[phase_idx] == 'cosine':
        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, cfg.epochs[phase_idx], eta_min=float(cfg.lr_min[phase_idx]))
    return optimizer, scheduler

# -----------------------------------------------------------------------
# æ ¸å¿ƒè®­ç»ƒå‡½æ•° (ç®€åŒ–ç‰ˆï¼Œåªå…³æ³¨ Phase 1)
# -----------------------------------------------------------------------
def main_train(cfg):
    utils.set_seed(cfg.seed)
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    default_wandb_project = 'arch_watermark_training'
    
    # 1. æ•°æ®è®¾ç½® - å…³é”®ä¿®æ”¹ç‚¹
    # å¿…é¡»æ¥æ”¶ utils.get_data_from_config è¿”å›çš„æ‰€æœ‰åŠ è½½å™¨ï¼Œä»¥é¿å… unpack é”™è¯¯
    try:
        data_loaders = utils.get_data_from_config(cfg) 
        
        # å‡è®¾æœ€å¸¸ç”¨çš„è¿”å›æ˜¯ 6 ä¸ª (train, test, wm, train_wm, ft, train_ft)
        if len(data_loaders) == 6:
            train_loader, test_loader, wm_loader, train_wm_loader, ft_loader, train_ft_loader = data_loaders
        elif len(data_loaders) == 2:
            # å¯¹åº” cfg.method == 'na' çš„æƒ…å†µ
            train_wm_loader, test_loader = data_loaders
            train_loader = train_wm_loader # å‡è®¾ train_wm_loader å¯ä»¥ä½œä¸ºå¸¸è§„ train_loader
            wm_loader = None
        else:
            # å…œåº•å¤„ç†ï¼Œç¡®ä¿è‡³å°‘æ‹¿åˆ° train_loader å’Œ test_loaderï¼Œå¹¶å°è¯•æ‹¿ wm_loader
            print(f"âš ï¸ Warning: utils.get_data_from_config returned {len(data_loaders)} loaders. Assuming the first three are clean_train, clean_test, and wm_test.")
            train_loader = data_loaders[0]
            test_loader = data_loaders[1]
            wm_loader = data_loaders[2] if len(data_loaders) > 2 else None

        classes = test_loader.dataset.classes
        num_classes = len(classes)
    except Exception as e:
        print(f"âŒ Error during data loading: {e}. Please ensure data_utils, utils, and all required data are configured.")
        return
    
    # 2. æ¨¡å‹åŠ è½½ (å…³é”®ä¿®æ”¹ç‚¹)
    try:
        ModelClass = load_arch_model(cfg.arch_key)
        net = ModelClass().to(device)
        print(f"âœ… Successfully loaded architecture model: {cfg.arch_key}")
    except ValueError as e:
        print(f"âŒ Error loading model: {e}")
        return
    
    print(f'\033[1m***** Training Architecture WM: {cfg.arch_key}, Target: {cfg.trigger_label} *****\033[0m')

    criterion = nn.CrossEntropyLoss()
    
    # æˆ‘ä»¬åªå…³æ³¨ '1_init' (åˆå§‹è®­ç»ƒ)
    phase_idx = 0 
    
    # è·å–ä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨ (ä½¿ç”¨é…ç½®ä¸­çš„ç¬¬ä¸€ç»„å‚æ•°)
    optimizer, scheduler = get_optims(net, cfg, phase_idx)
    
    # å®šä¹‰ä¿å­˜è·¯å¾„
    # å‘½åè§„åˆ™ï¼š[arch_key]_[target_label]_best.pth
    save_name = f"{cfg.arch_key}_{cfg.trigger_label}_best.pth"
    save_dir = os.path.join(cfg.save_dir, 'arch_weights')
    os.makedirs(save_dir, exist_ok=True)
    save_path = os.path.join(save_dir, save_name)

    # 3. Trainer åˆå§‹åŒ–å¹¶è®­ç»ƒ
    evaluator = Evaluator(net, criterion)
    
    # æ³¨æ„ï¼šæˆ‘ä»¬ä½¿ç”¨æ™®é€šçš„ Trainerï¼Œå› ä¸ºæ°´å°é€»è¾‘å·²åœ¨æ¨¡å‹ forward() ä¸­å®ç°
    # å¦‚æœæ‚¨çš„ Trainer éœ€è¦é¢å¤–çš„ wm_loaderï¼Œè¯·æ ¹æ®å®é™…æƒ…å†µè°ƒæ•´ data_utils.py æˆ–æ­¤å¤„çš„è°ƒç”¨
    trainer = Trainer(
        net=net, 
        criterion=criterion, 
        optimizer=optimizer, 
        evaluator=evaluator, 
        train_loader=train_loader, 
        test_loader=test_loader, 
        scheduler=scheduler
        # å‡è®¾æ‚¨çš„ Trainer æ”¯æŒ wm_loader=None
    )
    
    print(f"ğŸš€ Starting training for {cfg.epochs[phase_idx]} epochs...")
    # è°ƒç”¨è®­ç»ƒï¼Œå¹¶è®°å½•åˆ° WANDB
    trainer.train(
        exp_name=f"TRAIN_{cfg.arch_key}", 
        save_path=save_path, 
        epochs=cfg.epochs[phase_idx], 
        wandb_project=default_wandb_project, 
        use_wandb=cfg.log_wandb
    )
    
    print(f"âœ… Training complete. Weights saved to: {save_path}")
    print(f"WGT_PATH for evaluation is: {save_path}")


if __name__ == '__main__':
    parser = ArgumentParser()
    parser.add_argument('config', type=str, help='path to config file')
    args = parser.parse_args()
    
    try:
        with open(args.config) as f:
            cfg = EasyDict(yaml.safe_load(f))
            # ç¡®ä¿ epochs æ˜¯ä¸€ä¸ªåˆ—è¡¨ï¼Œä»¥åŒ¹é… get_optims çš„é€»è¾‘
            if not isinstance(cfg.epochs, list):
                cfg.epochs = [cfg.epochs] 
            if not isinstance(cfg.lr, list):
                cfg.lr = [cfg.lr]
            # ... å¯¹å…¶ä»–ç›¸ä½ç›¸å…³çš„é…ç½®è¿›è¡Œç±»ä¼¼å¤„ç†ï¼Œä»¥é€‚é… get_optims
            
    except FileNotFoundError:
        print(f"Error: Config file not found at {args.config}")
        exit()
        
    main_train(cfg)